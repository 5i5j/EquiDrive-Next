import numpy as np
import tensorflow as tf
from pathlib import Path
from sklearn.model_selection import train_test_split
# å¯¼å…¥å’±ä»¬åˆšåˆšå®šä¹‰çš„æ¨¡å‹
from src.models.baseline_v1 import build_v1_model

def load_silver_data(data_dir="data/silver_v1"):
    """
    å¤ä¹ ç‚¹ï¼šä» NPZ ä¸­æå–ç‰¹å¾å’Œæ ‡ç­¾
    """
    files = list(Path(data_dir).glob("*.npz"))
    x_data, y_data = [], []
    
    print(f"ğŸ“¦ æ­£åœ¨è½½å…¥ {len(files)} ä¸ªç™½é“¶æ ·æœ¬...")
    for f in files:
        data = np.load(f)
        x_data.append(data['x'])
        y_data.append(data['y'])
    
    return np.array(x_data), np.array(y_data)

def start_training():
    # 1. å‡†å¤‡æ•°æ®
    X, y = load_silver_data()
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

    # 2. æ„å»ºæ¨¡å‹
    model = build_v1_model(input_shape=(110, 2))
    
    # å¤ä¹ ç‚¹ï¼šä¸ºä»€ä¹ˆç”¨ SparseCategoricalCrossentropyï¼Ÿ
    # å› ä¸ºæˆ‘ä»¬çš„æ ‡ç­¾æ˜¯æ•°å­— (0, 1, 2)ï¼Œä¸æ˜¯ One-hot å‘é‡
    model.compile(
        optimizer='adam',
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

    # 3. å¼€å§‹ç‚¼ä¸¹
    print("\nğŸ”¥ P620 å¯åŠ¨ï¼Œå¼€å§‹ V1 Baseline è®­ç»ƒ...")
    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=50,
        batch_size=32
    )

    # 4. ä¿å­˜æ¨¡å‹ (Greenfield çš„ç¬¬ä¸€ä¸ªæ¨¡å‹èµ„äº§)
    model_path = Path("models/v1_baseline.h5")
    model_path.parent.mkdir(exist_ok=True)
    model.save(model_path)
    print(f"\nâœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜è‡³: {model_path}")

if __name__ == "__main__":
    start_training()